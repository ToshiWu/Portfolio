{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業目標：運用課程所學，操作字串達到預期輸出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 自行尋找一篇超過100字的文章\n",
    "    * 首先運用結巴斷詞，自行新增字典使得斷詞更為正確\n",
    "    * 使用jieba 完成 PoS Taggin\n",
    "    * 新增的詞也必須賦予詞性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "jieba.set_dictionary('./D11 data/dict.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "這是敘述近年來，AI 應用已無所不在，不論在新創或是傳產領域，都可能透過機器學習解決過去難以解決的問題。但目前台灣企業在 AI 導入的腳步仍然緩慢，除了人才嚴重短缺，教育資源無法即時跟上產業變異也是原因之一。因此，我們發起了「 機器學習 百日馬拉松 」教練陪跑計劃，翻轉傳統上課模式，以自主練習為主，幫助你獲得最大學習成效，搶先一步進入 AI 人工智能領域。\n"
     ]
    }
   ],
   "source": [
    "sentence = '''這是敘述近年來，AI 應用已無所不在，不論在新創或是傳產領域，都可能透過機器學習解決過去難以解決的問題。但目前台灣企業在 AI 導入的腳步仍然緩慢，除了人才嚴重短缺，教育資源無法即時跟上產業變異也是原因之一。因此，我們發起了「 機器學習 百日馬拉松 」教練陪跑計劃，翻轉傳統上課模式，以自主練習為主，幫助你獲得最大學習成效，搶先一步進入 AI 人工智能領域。'''\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\Home\\Files\\AI_suidies\\NLP50days\\homework\\D11 data\\dict.txt ...\n",
      "Loading model from cache C:\\Users\\user\\AppData\\Local\\Temp\\jieba.ue7d986a6a94eee5bff3cabb21ac128d4.cache\n",
      "Loading model cost 0.483 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output 精確模式: 這是|敘述|近年|來|，|AI| |應用|已|無所不在|，|不論|在|新創|或是|傳產|領域|，|都|可能|透過|機器|學習|解決|過去|難以|解決|的|問題|。|但|目前|台灣|企業|在| |AI| |導入|的|腳步|仍然|緩慢|，|除了|人才|嚴重|短缺|，|教育|資源|無法|即時|跟上|產業|變異|也|是|原因|之|一|。|因此|，|我們|發起|了|「| |機器|學習| |百日|馬拉松| |」|教練|陪跑|計劃|，|翻轉|傳統|上課|模式|，|以|自主|練習|為主|，|幫助|你|獲得|最|大|學習|成效|，|搶先|一步|進入| |AI| |人工|智能|領域|。\n"
     ]
    }
   ],
   "source": [
    "print(\"output 精確模式: {}\".format('|'.join(jieba.cut(sentence, cut_all=False, HMM=False))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新增字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 讀入字典\n",
    "jieba.load_userdict('./D11 data/adding.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output 精確模式: 這是|敘述|近年|來|，|AI| |應用|已|無所不在|，|不論|在|新創|或是|傳產|領域|，|都|可能|透過|機器|學習|解決|過去|難以|解決|的|問題|。|但|目前|台灣|企業|在| |AI| |導入|的|腳步|仍然|緩慢|，|除了|人才|嚴重|短缺|，|教育|資源|無法|即時|跟上|產業|變異|也|是|原因|之|一|。|因此|，|我們|發起|了|「| |機器|學習| |百日|馬拉松| |」|教練|陪跑|計劃|，|翻轉|傳統|上課|模式|，|以|自主|練習|為主|，|幫助|你|獲得|最|大|學習|成效|，|搶先|一步|進入| |AI| |人工|智能|領域|。\n"
     ]
    }
   ],
   "source": [
    "print(\"output 精確模式: {}\".format('|'.join(jieba.cut(sentence, cut_all=False, HMM=False))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "這是   x            \t敘述   x            \t近年   t            \t來   zg            \tAI   eng          \n",
      "應用   x            \t已   d             \t無所不在   x          \t不論   x            \t在   p             \n",
      "新創   x            \t或是   c            \t傳產   x            \t領域   x            \t都   d             \n",
      "可能   v            \t透過   x            \t機器   x            \t學習   x            \t解決   x            \n",
      "過去   x            \t難以   x            \t解決   x            \t的   uj            \t問題   x            \n",
      "但   c             \t目前   t            \t台灣   x            \t企業   x            \t在   p             \n",
      "AI   eng          \t導入   x            \t的   uj            \t腳步   x            \t仍然   d            \n",
      "緩慢   x            \t除了   p            \t人才   n            \t嚴重   x            \t短缺   a            \n",
      "教育   vn           \t資源   x            \t無法   x            \t即時   x            \t跟上   f            \n",
      "產業   x            \t變異   x            \t也   d             \t是   v             \t原因   n            \n",
      "之一   r            \t因此   c            \t我們   x            \t發起   x            \t了   ul            \n",
      "機器   x            \t學習   x            \t百日   m            \t馬拉松   x           \t教練   x            \n",
      "陪跑   x            \t計劃   x            \t翻轉   x            \t傳統   x            \t上課   x            \n",
      "模式   n            \t以   p             \t自主   v            \t練習   x            \t為主   x            \n",
      "幫助   x            \t你   r             \t獲得   x            \t最   d             \t大   a             \n",
      "學習   x            \t成效   a            \t搶先   x            \t一步   m            \t進入   x            \n",
      "AI   eng          \t人工   n            \t智能   n            \t領域   x            \t"
     ]
    }
   ],
   "source": [
    "words = pseg.cut(sentence,)\n",
    "ng_word = ' ，。「」'\n",
    "count = 1\n",
    "for word ,flag in words:\n",
    "    if word not in ng_word:\n",
    "        print(f'{word + \"   \" + flag:<18}', end = '\\t' if count % 5 else '\\n')\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: \n",
      "Part-of-speech|tagging|In|any|real|test|,|part-of-speech|tagging|and|sense|tagging|having|been|proven|to|be|very|closely|related|with|each|potentially|making|constraints|to|the|other|.|The|question|whether|these|tasks|should|be|kept|together|or|decoupled|is|still|not|unanimously|resolved|,|but|recently|scientists|incline|to|test|these|things|separately|(|e.g|.|in|the|Senseval/SemEval|competitions|parts|of|speech|are|provided|as|input|for|the|text|to|disambiguate|)|.|Both|WSM|part-of-speech|tagging|involve|disambiguating|or|tagging|with|words|.|However|,|algorithms|used|for|one|do|not|tend|to|work|well|for|the|other|,|mainly|because|the|part|of|speech|of|a|word|is|primarily|determined|by|the|immediately|adjacent|one|to|three|words|,|whereas|the|sense|of|a|word|may|be|determined|by|words|further|away|.|The|success|rate|for|part-of-speech|tagging|algorithms|is|at|present|much|higher|than|that|for|WSD|,|state-of-the|art|being|around|96|%|accuracy|or|better|,|as|compared|to|less|than|75|%|accuracy|in|word|sense|disambiguation|with|supervised|learning|.|These|figures|are|typical|for|English|,|and|may|be|very|different|from|those|for|other|languages|.\n",
      "\n",
      "\n",
      "\n",
      "Part-of-speech   JJ \ttagging   NN        \tIn   IN             \tany   DT            \treal   JJ           \n",
      "test   NN           \t,   ,               \tpart-of-speech   JJ \ttagging   NN        \tand   CC            \n",
      "sense   NN          \ttagging   VBG       \thaving   VBG        \tbeen   VBN          \tproven   VBN        \n",
      "to   TO             \tbe   VB             \tvery   RB           \tclosely   RB        \trelated   VBN       \n",
      "with   IN           \teach   DT           \tpotentially   RB    \tmaking   VBG        \tconstraints   NNS   \n",
      "to   TO             \tthe   DT            \tother   JJ          \t.   .               \tThe   DT            \n",
      "question   NN       \twhether   IN        \tthese   DT          \ttasks   NNS         \tshould   MD         \n",
      "be   VB             \tkept   VBN          \ttogether   RB       \tor   CC             \tdecoupled   VBN     \n",
      "is   VBZ            \tstill   RB          \tnot   RB            \tunanimously   RB    \tresolved   VBN      \n",
      ",   ,               \tbut   CC            \trecently   RB       \tscientists   NNS    \tincline   VBP       \n",
      "to   TO             \ttest   VB           \tthese   DT          \tthings   NNS        \tseparately   RB     \n",
      "(   (               \te.g   NN            \t.   .               \tin   IN             \tthe   DT            \n",
      "Senseval/SemEval   NNP\tcompetitions   NNS  \tparts   NNS         \tof   IN             \tspeech   NN         \n",
      "are   VBP           \tprovided   VBN      \tas   IN             \tinput   NN          \tfor   IN            \n",
      "the   DT            \ttext   NN           \tto   TO             \tdisambiguate   VB   \t)   )               \n",
      ".   .               \tBoth   DT           \tWSM   NNP           \tpart-of-speech   NN \ttagging   VBG       \n",
      "involve   VB        \tdisambiguating   VBG\tor   CC             \ttagging   VBG       \twith   IN           \n",
      "words   NNS         \t.   .               \tHowever   RB        \t,   ,               \talgorithms   NNS    \n",
      "used   VBD          \tfor   IN            \tone   CD            \tdo   NN             \tnot   RB            \n",
      "tend   VB           \tto   TO             \twork   VB           \twell   RB           \tfor   IN            \n",
      "the   DT            \tother   JJ          \t,   ,               \tmainly   RB         \tbecause   IN        \n",
      "the   DT            \tpart   NN           \tof   IN             \tspeech   NN         \tof   IN             \n",
      "a   DT              \tword   NN           \tis   VBZ            \tprimarily   RB      \tdetermined   VBN    \n",
      "by   IN             \tthe   DT            \timmediately   RB    \tadjacent   JJ       \tone   CD            \n",
      "to   TO             \tthree   CD          \twords   NNS         \t,   ,               \twhereas   IN        \n",
      "the   DT            \tsense   NN          \tof   IN             \ta   DT              \tword   NN           \n",
      "may   MD            \tbe   VB             \tdetermined   VBN    \tby   IN             \twords   NNS         \n",
      "further   RB        \taway   RB           \t.   .               \tThe   DT            \tsuccess   NN        \n",
      "rate   NN           \tfor   IN            \tpart-of-speech   JJ \ttagging   NN        \talgorithms   NN     \n",
      "is   VBZ            \tat   IN             \tpresent   JJ        \tmuch   RB           \thigher   JJR        \n",
      "than   IN           \tthat   DT           \tfor   IN            \tWSD   NNP           \t,   ,               \n",
      "state-of-the   JJ   \tart   NN            \tbeing   VBG         \taround   RB         \t96   CD             \n",
      "%   NN              \taccuracy   NN       \tor   CC             \tbetter   JJR        \t,   ,               \n",
      "as   IN             \tcompared   VBN      \tto   TO             \tless   JJR          \tthan   IN           \n",
      "75   CD             \t%   NN              \taccuracy   NN       \tin   IN             \tword   NN           \n",
      "sense   NN          \tdisambiguation   NN \twith   IN           \tsupervised   JJ     \tlearning   NN       \n",
      ".   .               \tThese   DT          \tfigures   NNS       \tare   VBP           \ttypical   JJ        \n",
      "for   IN            \tEnglish   NNP       \t,   ,               \tand   CC            \tmay   MD            \n",
      "be   VB             \tvery   RB           \tdifferent   JJ      \tfrom   IN           \tthose   DT          \n",
      "for   IN            \tother   JJ          \tlanguages   NNS     \t.   .               \t"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# 需先下載 punkt 和 averaged_perceptron_tagger\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "sentence = '''Part-of-speech tagging\n",
    "In any real test, part-of-speech tagging and sense tagging having been proven to be very closely related with each potentially making constraints to the other.\n",
    "The question whether these tasks should be kept together or decoupled is still not unanimously resolved, but recently scientists incline to test these things separately (e.g. in the Senseval/SemEval competitions parts of speech are provided as input for the text to disambiguate).\n",
    "\n",
    "Both WSM part-of-speech tagging involve disambiguating or tagging with words.\n",
    "However, algorithms used for one do not tend to work well for the other, mainly because the part of speech of a word is primarily determined by the immediately adjacent one to three words, whereas the sense of a word may be determined by words further away. The success rate for part-of-speech tagging algorithms is at present much higher than that for WSD, state-of-the art being around 96% accuracy or better, as compared to less than 75% accuracy in word sense disambiguation with supervised learning.\n",
    "These figures are typical for English, and may be very different from those for other languages.'''\n",
    "\n",
    "tokenize = nltk.word_tokenize(sentence)\n",
    "print(f'Token: \\n{\"|\".join(tokenize)}\\n\\n\\n')\n",
    "tagging = nltk.pos_tag(tokenize,)\n",
    "count = 1\n",
    "\n",
    "for tag in tagging:\n",
    "    print(f'{tag[0] + \"   \" + tag[1]:<20}', end = '\\t' if count % 5 else '\\n')\n",
    "    count+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
