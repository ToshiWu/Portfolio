{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作業目的:了解決策樹的節點分支依據\n",
    "本次作業可參考簡報中的延伸閱讀[訊息增益](https://medium.com/jameslearningnote/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC3-5%E8%AC%9B-%E6%B1%BA%E7%AD%96%E6%A8%B9-decision-tree-%E4%BB%A5%E5%8F%8A%E9%9A%A8%E6%A9%9F%E6%A3%AE%E6%9E%97-random-forest-%E4%BB%8B%E7%B4%B9-7079b0ddfbda)部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question:\n",
    "若你是決策樹，下列兩種分類狀況(a,b)，你會選擇哪種做分類？為什麼？\n",
    "\n",
    "<img src='D24 決策樹演算法(Decision Tree).png' style='width:500px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "\n",
    "##### 　　　(a)，因為訊息增益較強。\n",
    "\\begin{aligned}\n",
    "&\\text {Gain by Entropy of (a)} \\\\\n",
    "&I_{H}(D_{p})=-(\\frac{1}{2}*\\log_{2}\\frac{1}{2}+\\frac{1}{2}*\\log_{2}\\frac{1}{2})=1\\\\\n",
    "&I_{H}(D_{left})=-(\\frac{3}{4}*\\log_{2}\\frac{3}{4}+\\frac{1}{4}*\\log_{2}\\frac{1}{4})=0.81\\\\\n",
    "&I_{H}(D_{right})=-(\\frac{1}{4}*\\log_{2}\\frac{1}{4}+\\frac{3}{4}*\\log_{2}\\frac{3}{4})=0.81\\\\\n",
    "&IG_{H}=1-\\frac{1}{2}*0.81-\\frac{1}{2}*0.81=0.19\\\\\n",
    "\\\\\n",
    "&\\text {Gain by Entropy of (b)} \\\\\n",
    "&I_{H}(D_{p})=-(\\frac{1}{2}*\\log_{2}\\frac{1}{2}+\\frac{1}{2}*\\log_{2}\\frac{1}{2})=1\\\\\n",
    "&I_{H}(D_{left})=-(\\frac{3}{7}*\\log_{2}\\frac{3}{7}+\\frac{4}{7}*\\log_{2}\\frac{4}{7})=0.985\\\\\n",
    "&I_{H}(D_{right})=-(1*\\log_{2}1+0*\\log_{2}0)=0\\\\\n",
    "&IG_{H}=1-\\frac{7}{8}*0.985-\\frac{1}{8}*0=0.138\\\\\n",
    "\\\\\n",
    "&\\text {Gain by Gini of (a)} \\\\\n",
    "&I_{G}(D_{p})=1-((\\frac{1}{2})^{2})+(\\frac{1}{2})^{2}))=0.5\\\\\n",
    "&I_{G}(D_{left})=1-((\\frac{3}{4})^{2})+(\\frac{1}{4})^{2}))=0.375\\\\\n",
    "&I_{G}(D_{right})=1-((\\frac{1}{4})^{2})+(\\frac{3}{4})^{2}))=0.375\\\\\n",
    "&I_{G}=0.5-\\frac{1}{2}*0.375-\\frac{1}{2}*0.375=0.125\\\\\n",
    "\\\\\n",
    "&\\text {Gain by Gini of (b)} \\\\\n",
    "&I_{G}(D_{p})=1-((\\frac{1}{2})^{2})+(\\frac{1}{2})^{2}))=0.5\\\\\n",
    "&I_{G}(D_{left})=1-((\\frac{3}{7})^{2})+(\\frac{4}{7})^{2}))=0.49\\\\\n",
    "&I_{G}(D_{right})=1-(1^{2}+0^{2})=0\\\\\n",
    "&I_{G}=0.5-\\frac{7}{8}*0.49-\\frac{1}{8}*0=0.071\\\\\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 閱讀作業\n",
    "\n",
    "決策樹根據計算分割準則的不同(ex: Entropy, Gini, Gain ratio)，可分為ID3, C4.5, CART樹的算法，請同學閱讀下列文章，來更加了解決策樹的算法。\n",
    "\n",
    "[決策樹(ID3, C4.5, CART)](https://blog.csdn.net/u010089444/article/details/53241218)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
